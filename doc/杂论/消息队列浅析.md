[TOC]

# 消息队列浅析

## 什么是消息队列

### 概述

> **消息队列**（Message Queue）是一种[进程间通信](https://zh.wikipedia.org/wiki/进程间通信)或同一进程的不同[线程](https://zh.wikipedia.org/wiki/线程)间的通信方式。

+ 消息（Message）：计算机系统中的一种数据传输**单位**。

+ 队列（Queue）：一种**数据结构**，具有**先进先出**的特性。

+ 生产者/发布者（Producer）：生成消息并将消息存入消息队列。

+ 消费者/订阅者（Consumer）：从消息队列中取出消息进行处理。

  <img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yapueswj30mp09qjrv.jpg" alt="消息队列" style="zoom:100%;" />

### 工作模式

+ 点对点

  + 生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有 一个可用的消费者。

    ![点对点模式](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yaw39r1j30ev09pmxl.jpg)

+ 发布/订阅

  + 发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。

    ![发布订阅模式](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yb0xsobj30ev09p0t9.jpg)

## 为什么需要使用消息队列

先来看看下面的这些场景

### 场景一

+ 宝洁OLAY添加好友回调结束后，假如我们需要将好友关系同步给CRM、Loyaty，过了几天新增需求要把好友关系同步给AM，又过了几天又有了新需求不需要把好友关系同步给CRM了，又过了几天新的需求需要把好友关系同步给DMP，又过了几天......

  ![解耦前](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yb519uoj30hi0b33z3.jpg)


### 场景二

+ 某网站用户注册成功时需要给用户发送通知短息和邮件，假设用户请求请求注册需要10ms，本地处理数据入库需要50ms，发送短信和邮件各需要100ms

  ![异步前](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yba55s5j30dw0c5wf3.jpg)

  这一整个流程下来 10ms + 50ms + 100ms + 100ms = 260ms

  这个时候有用户开始抱怨了，我搞个注册竟然要等这么久（虽然不是很久。。。），不搞了，走了。

### 场景三

+ 一个不知名的电商平台，系统能承受的最大请求数是4000个/s。

  ![削峰前](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6ybf4h1oj30bg06hjrk.jpg)

  突然有一天想要搞个秒杀活动，然后呢，因为前期宣传到位，参与的用户挺多；活动刚开始，页面就出现了这样

  ![网络忙](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6ybk2li1j30u00hadgo.jpg)

  

  运维小哥一查发现是系统瞬时请求量过大导致系统崩掉了，这个时候程序员小哥哥慌了。

### 解决一下

这几个场景是开发中经常会遇到的问题，大家在开发中会怎样去处理这些问题呢？

......

接下来看看咱们今天的主角**消息队列**是如何解决这些问题的

+ 场景一解决方案

  添加好友回调后，我们把回调信息写入到MQ中，接下来就是各个系统自己去取数据，然后再做各自的逻辑处理，这样就算有新的系统接入，我们也可以在不改动代码的情况下实现功能。

  ![解耦后](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yboke0vj30lr0g8t9x.jpg)

+ 场景二解决方案

  用户注册后把注册信息写入消息队列中，然后通知短信、邮件这个功能去做各自的业务处理，这样一来响应耗时从原来的260ms变为20ms，提高了10倍多。

  ![异步后](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6ybsfgeij30f9099jry.jpg)

+ 场景三解决方案

  瞬时请求量过高的情况下，我们可以将请求数据暂时存入到消息队列中，利用消息队列做一个缓冲，业务处理时只用从消息队列中取数据，这样就极大的减轻了业务处理系统的压力。
  
  ![削峰后](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6ybw6prgj30bd09874j.jpg)

### 小结一下

+ 以上的三个场景是开发中经常会遇到的问题：高耦合、请求-响应耗时长、请求骤增导致业务压力过大，使用消息队列可以完美的解决。

  消息队列具有解耦合、异步处理和限流削峰等特点。

## 消息队列存在的问题

### 增加系统复杂性&降低系统稳定性

+ “链路越长，出错的几率就越大” ，如果出错了就会导致整个系统不可用

  ![复杂性&稳定性](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yc1sl19j30hw07zq39.jpg)
  
  解决方案：集群/分布式

### 顺序消费

+ 发送有序，消费不一定有序

  ![顺序消费&一致性问题](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6yca8vilj30et07swep.jpg)
  
  解决方案：生产者保证顺序发送，**消费者保证顺序消费**

### 重复消费

+ 异步通知一定会有重试机制，这就会存在同一消息重复消费的情况

  ![重复消费](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6ycgf4l5j30qe0d6jry.jpg)

  解决方案：数据幂等

  强校验-事务处理、弱检验-缓存处理

### 消息可靠性&数据丢失

+ 生产者数据丢失：网络原因导致数据未成功写入消息队列

  解决方案：事务机制（同步）或confirm机制（异步）

+ 消息队列数据丢失

  解决方案：持久化（不能保证100%不丢失）

+ 消费者数据丢失：消费端处理的时候未完成处理，进程挂掉了

  解决方案：客户端ack机制

## 常用消息队列对比

|                        | Kafka                                | RabbitMQ                                     | ZeroMQ                   | RocketMQ               | ActiveMQ                             |
| ---------------------- | ------------------------------------ | -------------------------------------------- | ------------------------ | ---------------------- | ------------------------------------ |
| **开发语言**           | Scala                                | Erlang                                       | C                        | Java                   | Java                                 |
| **消息存储**           | 内存、磁盘、数据库<br />支持大量堆积 | 内存、磁盘<br />支持少量堆积                 | 内存、磁盘               | 磁盘<br />支持大量堆积 | 内存、磁盘、数据库<br />支持少量堆积 |
| **消息事务**           | 支持                                 | 支持                                         | 不支持                   | 支持                   | 支持                                 |
| **负载均衡**           | 分摊负载、主动完成                   | 可手动完成，<br />受限于交换器、路由键、队列 | 去中心化，不支持负载均衡 | 支持负载均衡、分摊负载 | 支持负载均衡，基于zookeeper          |
| **集群方式**           | “Leader-Slave”无状态集群             | “复制”模式的简单集群                         | 去中心化，不支持集群     | 多对“Master-Slave”模式 | 简单集群模式，对高级集群模式不友好   |
| **可用性**             | 非常高（分布式）                     | 高（主从）                                   | 高                       | 非常高（分布式）       | 高（主从）                           |
| **消息重复**           | 支持at least once、at most once      | 支持at least once、at most once              | -                        | 支持at least once      | 支持at least once                    |
| **订阅形式和消息分发** | 发布/订阅                            | 点对点、发布/订阅                            | 点对点                   | 发布/订阅              | 点对点、发布/订阅                    |
| **时效性**             | ms 级                                | μs级                                         | 号称“零延迟”             | ms 级                  | ms 级                                |
| **顺序消息**           | 支持                                 | 不支持                                       | 不支持                   | 支持                   | 不支持                               |
| **消息确认**           | 支持                                 | 支持                                         | 支持                     | 支持                   | 支持                                 |
| **消息回溯**           | 指定分区的消息回溯                   | 不支持                                       | 不支持                   | 指定时间点的回溯       | 不支持                               |
| **消息重试**           | 不支持，可手动实现（消息回溯）       | 不支持，可手动实现（消息确认机制）           | 不支持                   | 支持（定时重试）       | 不支持                               |
| **并发度**             | 并发度高                             | 并发度极高                                   | 并发度高                 | 并发度高               | 并发度高                             |



参考资料：

[消息队列及常见消息队列介绍](https://cloud.tencent.com/developer/article/1006035)

[消息队列（mq）是什么？](https://www.zhihu.com/question/54152397)

[常用消息队列对比](https://dbaplus.cn/news-159-2611-1.html)

[消息队列Kafka、RocketMQ、RabbitMQ的优劣势比较](https://zhuanlan.zhihu.com/p/60288391)

[深入浅出PHP消息队列](https://www.kancloud.cn/vson/php-message-queue/885553)


